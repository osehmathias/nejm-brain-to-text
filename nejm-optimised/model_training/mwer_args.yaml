# MWER Fine-tuning Configuration
# This config is used for Minimum Word Error Rate fine-tuning
# Starting from a pretrained CTC model

# Inherit base model config
model:
  n_input_features: 512
  n_units: 768
  rnn_dropout: 0.0  # No dropout during MWER fine-tuning
  rnn_trainable: true
  n_layers: 5
  patch_size: 14
  patch_stride: 4

  input_network:
    n_input_layers: 1
    input_layer_sizes:
    - 512
    input_trainable: false  # Freeze day layers during MWER fine-tuning
    input_layer_dropout: 0.0

gpu_number: '0'
mode: mwer  # MWER fine-tuning mode
use_amp: true

# Output directories
output_dir: trained_models/mwer_finetuned
checkpoint_dir: trained_models/mwer_finetuned/checkpoint

# Initialize from pretrained CTC checkpoint
init_from_checkpoint: true
init_checkpoint_path: ../data/t15_pretrained_rnn_baseline/checkpoint/best_checkpoint

# Checkpoint settings
save_best_checkpoint: true
save_all_val_steps: false
save_final_model: true
save_val_metrics: true
early_stopping: true
early_stopping_val_steps: 10  # Stop if no improvement for 10 validation steps

# MWER Training parameters
num_training_batches: 20000  # Fewer batches than CTC training
mwer:
  beam_size: 8  # N-best beam size for MWER
  sample_size: 4  # Number of samples for policy gradient
  baseline_type: 'mean'  # 'mean', 'min', or 'none'
  wer_weight: 1.0  # Weight for WER loss
  ctc_weight: 0.1  # Optional auxiliary CTC loss weight for stability
  temperature: 1.0  # Temperature for sampling
  use_sampling: true  # Use sampling instead of argmax for diversity

# Much smaller learning rate for fine-tuning
lr_scheduler_type: cosine
lr_max: 0.00001  # 1e-5, much smaller than CTC training
lr_min: 0.000001  # 1e-6
lr_decay_steps: 20000
lr_warmup_steps: 500
lr_max_day: 0.0  # Frozen day layers
lr_min_day: 0.0
lr_decay_steps_day: 20000
lr_warmup_steps_day: 500

# Optimizer parameters
beta0: 0.9
beta1: 0.999
epsilon: 0.1
weight_decay: 0.0001  # Smaller weight decay for fine-tuning
weight_decay_day: 0
seed: 42
grad_norm_clip_value: 1.0  # Smaller gradient clipping for stability

# Logging
batches_per_train_log: 100
batches_per_val_step: 1000  # More frequent validation for MWER

batches_per_save: 0
log_individual_day_val_PER: true
log_val_skip_logs: false
save_val_logits: false  # Don't save logits to reduce disk usage
save_val_data: false

# Dataset - same as CTC training
dataset:
  data_transforms:
    white_noise_std: 0.0  # No augmentation during MWER fine-tuning
    constant_offset_std: 0.0
    random_walk_std: 0.0
    random_walk_axis: -1
    static_gain_std: 0.0
    random_cut: 0
    smooth_kernel_size: 100
    smooth_data: true
    smooth_kernel_std: 2

  neural_dim: 512
  batch_size: 16  # Smaller batch size for MWER (memory intensive)
  n_classes: 41
  max_seq_elements: 500
  days_per_batch: 4
  seed: 1
  num_dataloader_workers: 4
  loader_shuffle: false
  must_include_days: null
  test_percentage: 0.1
  feature_subset: null

  dataset_dir: ../data/hdf5_data_final
  bad_trials_dict: null
  sessions:
  - t15.2023.08.11
  - t15.2023.08.13
  - t15.2023.08.18
  - t15.2023.08.20
  - t15.2023.08.25
  - t15.2023.08.27
  - t15.2023.09.01
  - t15.2023.09.03
  - t15.2023.09.24
  - t15.2023.09.29
  - t15.2023.10.01
  - t15.2023.10.06
  - t15.2023.10.08
  - t15.2023.10.13
  - t15.2023.10.15
  - t15.2023.10.20
  - t15.2023.10.22
  - t15.2023.11.03
  - t15.2023.11.04
  - t15.2023.11.17
  - t15.2023.11.19
  - t15.2023.11.26
  - t15.2023.12.03
  - t15.2023.12.08
  - t15.2023.12.10
  - t15.2023.12.17
  - t15.2023.12.29
  - t15.2024.02.25
  - t15.2024.03.03
  - t15.2024.03.08
  - t15.2024.03.15
  - t15.2024.03.17
  - t15.2024.04.25
  - t15.2024.04.28
  - t15.2024.05.10
  - t15.2024.06.14
  - t15.2024.07.19
  - t15.2024.07.21
  - t15.2024.07.28
  - t15.2025.01.10
  - t15.2025.01.12
  - t15.2025.03.14
  - t15.2025.03.16
  - t15.2025.03.30
  - t15.2025.04.13
  dataset_probability_val:
  - 0
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 0
  - 1
  - 1
  - 1
  - 0
  - 0
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
